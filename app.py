import os
import re
import io
import streamlit as st
import pandas as pd
import altair as alt
import pymysql
from sqlalchemy import create_engine, text
from sqlalchemy import types as satypes
from sqlalchemy.exc import SQLAlchemyError
from openai import OpenAI
import time
from sqlalchemy import inspect, MetaData, Table
from sqlalchemy.dialects.mysql import insert as mysql_insert
from decimal import Decimal

# ==========================
# CONFIG FROM ENVIRONMENT
# ==========================
OPENROUTER_API_KEY = "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"
OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"
OPENROUTER_MODEL = "gpt-4o-mini"

DB_USER = os.getenv("DB_USER", "test")
DB_PASS = os.getenv("DB_PASS", "test123")
DB_HOST = os.getenv("DB_HOST", "91.108.105.168")
DB_NAME = os.getenv("DB_NAME", "SmartDB")

# SQLAlchemy engine
engine = create_engine(f"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME}")

# ==========================
# HELPERS
# ==========================
def get_schema_str(engine) -> str:
    schema = []
    with engine.connect() as conn:
        result = conn.execute(text("SHOW TABLES;"))
        tables = [row[0] for row in result]
        for tname in tables:
            schema.append(f"Table {tname}:")
            cols = conn.execute(text(f"SHOW COLUMNS FROM {tname};"))
            for col in cols:
                schema.append(f" - {col[0]} ({col[1]})")
    return "\n".join(schema)

def strip_code_fences(text: str) -> str:
    text = text.strip()
    m = re.search(r"```(?:sql)?\s*(.*?)\s*```", text, flags=re.IGNORECASE | re.DOTALL)
    if m:
        return m.group(1).strip()
    return text

def is_readonly_select(sql: str) -> bool:
    s = sql.strip().strip(";").lower()
    if not re.match(r"^(with\s+.*\)\s*)?select\b", s, flags=re.DOTALL):
        return False
    dangerous = ["insert", "update", "delete", "drop", "alter", "create ",
                 "truncate", "attach", "detach", "vacuum"]
    return not any(re.search(rf"\b{kw}\b", s) for kw in dangerous)

def split_sql_statements(sql: str):
    parts = [p.strip() for p in sql.split(";")]
    return [p for p in parts if p]

def df_to_excel_bytes(df: pd.DataFrame) -> bytes:
    buf = io.BytesIO()
    try:
        with pd.ExcelWriter(buf, engine="xlsxwriter") as writer:
            df.to_excel(writer, index=False)
        buf.seek(0)
        return buf.read()
    except Exception:
        return df.to_csv(index=False).encode("utf-8")

def generate_sql(prompt: str, schema: str) -> str:
    client = OpenAI(base_url=OPENROUTER_BASE_URL, api_key=OPENROUTER_API_KEY)
    system_prompt = f"""
You are an expert SQL assistant. The database schema is:
{schema}

Rules:
- Return only RAW MySQL/MariaDB SQL (no markdown, no code fences).
- Prefer simple, single-statement SELECT queries unless user explicitly asks for multiple results.
"""
    resp = client.chat.completions.create(
        model=OPENROUTER_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt},
        ],
        temperature=0,
    )
    choice = resp.choices[0]
    content = ""
    if hasattr(choice, "message") and choice.message and hasattr(choice.message, "content"):
        content = choice.message.content or ""
    elif isinstance(choice, dict):
        content = choice.get("message", {}).get("content", "") or ""
    else:
        content = str(resp)
    sql = strip_code_fences(content)
    if not sql:
        raise ValueError("No SQL generated by the model.")
    return sql

# ==========================
# MAIN APP
# ==========================
def main():
    st.set_page_config(page_title="AI Database Explorer", page_icon="üóÑÔ∏è", layout="wide")
    st.title("üóÑÔ∏è AI-Powered MariaDB Explorer")
    st.caption("Ask your SmartDB database in natural language. Read-only & safe.")

    # --------------------
    # Sidebar: Schema
    # --------------------
    with st.sidebar:
        st.header("üìë Database Schema")
        schema_text = get_schema_str(engine)
        for block in schema_text.split("\nTable "):
            if not block.strip():
                continue
            lines = block.split("\n")
            tname = lines[0].replace("Table ", "")
            with st.expander(f"Table: {tname}", expanded=False):
                for col in lines[1:]:
                    st.write(col.strip())
        st.markdown("---")

    # --------------------
    # AI Query Section
    # --------------------
    user_query = st.text_input("üîç Your question (e.g., 'show all customers', 'total order amount by customer')")
    if "history" not in st.session_state:
        st.session_state.history = []

    run = st.button("‚ñ∂Ô∏è Run Query")
    if run:
        if not user_query.strip():
            st.error("Please enter a query.")
        else:
            with st.spinner("ü§ñ Generating SQL..."):
                try:
                    sql_text = generate_sql(user_query, schema_text)
                except Exception as e:
                    st.error(f"AI error: {e}")
                    return

            st.code(sql_text, language="sql")
            statements = split_sql_statements(sql_text)
            if not statements:
                st.warning("No SQL statements to execute.")
                return

            tabs = st.tabs([f"Result {i+1}" for i in range(len(statements))])
            for i, (tab, stmt) in enumerate(zip(tabs, statements)):
                with tab:
                    if not is_readonly_select(stmt):
                        st.error("‚ùå Only read-only SELECT queries are allowed.")
                        continue
                    try:
                        df = pd.read_sql(text(stmt), engine)
                    except Exception as e:
                        st.error(f"Query failed: {e}")
                        continue

                    if df.empty:
                        st.warning("‚ö†Ô∏è No results found.")
                    else:
                        st.success(f"‚úÖ {len(df)} rows")
                        st.dataframe(df, use_container_width=True)

                        # Downloads
                        st.download_button("‚¨áÔ∏è Download CSV", data=df.to_csv(index=False).encode("utf-8"),
                                           file_name=f"results_{i+1}.csv", mime="text/csv")
                        st.download_button("‚¨áÔ∏è Download Excel", data=df_to_excel_bytes(df),
                                           file_name=f"results_{i+1}.xlsx",
                                           mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

                        # Quick Charts
                        num_cols = df.select_dtypes(include=["number"]).columns.tolist()
                        if num_cols:
                            st.subheader("üìä Quick Charts")
                            chart_choice = st.radio("Chart type", ["Bar", "Line"], horizontal=True, key=f"chart_{i}")
                            if chart_choice == "Bar":
                                st.bar_chart(df[num_cols])
                            else:
                                melted = df.reset_index().melt(
                                    id_vars=["index"], value_vars=num_cols,
                                    var_name="Column", value_name="Value"
                                )
                                chart = (
                                    alt.Chart(melted)
                                    .mark_line(point=True)
                                    .encode(
                                        x="index:O",
                                        y="Value:Q",
                                        color="Column:N",
                                        tooltip=["Column", "Value"]
                                    )
                                )
                                st.altair_chart(chart, use_container_width=True)

            st.session_state.history.append({"query": user_query, "sql": sql_text})

    # --------------------
    # History Section
    # --------------------
    if st.session_state.history:
        st.subheader("üïí Recent Queries")
        for h in st.session_state.history[-5:][::-1]:
            st.markdown(f"**Query:** {h['query']}")
            st.code(h["sql"], language="sql")
            st.markdown("---")

    # --------------------
    # CSV Upload Section
    # --------------------
    st.markdown("---")
    st.header("üì• Upload CSV to MariaDB")

    uploaded_file = st.file_uploader("Upload CSV file", type=["csv"])
    if uploaded_file:
        df_upload = pd.read_csv(uploaded_file)
        st.write("Preview of uploaded data:", df_upload.head())

        # Normalize incoming column labels
        incoming_cols_original = list(df_upload.columns)
        df_upload.columns = [c.strip() for c in df_upload.columns]

        # Convert blanks to None
        df_upload = df_upload.replace({"": None, "nan": None, "NaN": None})
        df_upload = df_upload.where(pd.notnull(df_upload), None)

        # Select target table
        with engine.connect() as conn:
            result = conn.execute(text("SHOW TABLES;"))
            tables = [row[0] for row in result]
        target_table = st.selectbox("Select target table", tables)
        skip_duplicates = st.checkbox("Skip duplicate key rows", value=True)
        skip_empty_rows = st.checkbox("Skip rows with mostly empty values", value=True)
        min_populated_fields = st.slider("Minimum populated fields required per row", 1, 5, 2)

        if st.button("‚û°Ô∏è Insert CSV into table"):
            if not target_table:
                st.error("Please select a table.")
            else:
                try:
                    inspector = inspect(engine)
                    cols_info = inspector.get_columns(target_table)
                    existing_cols_original = [c['name'] for c in cols_info]

                    # Normalization + synonyms
                    def norm_with_synonyms(name: str) -> str:
                        name = name.strip().lower()
                        replacements = {
                            "donar": "donor", "devotte": "devotee",
                            "recipt": "receipt", "reciept": "receipt",
                            "pancard": "pan", "pooja": "puja",
                            "pin code": "pincode", "pincodenumber": "pincode",
                            "mobile number": "mobile",
                            "amount of donation": "amount",
                        }
                        for src, dst in replacements.items():
                            name = name.replace(src, dst)
                        return re.sub(r"[^a-z0-9]", "", name)

                    def token_set(name: str) -> set:
                        return set(re.findall(r"[a-z0-9]+", name.lower()))

                    normalized_to_dbcol = {norm_with_synonyms(c): c for c in existing_cols_original}

                    # First pass match
                    rename_map, matched_cols, unmatched = {}, [], []
                    for c in df_upload.columns:
                        nc = norm_with_synonyms(c)
                        if nc in normalized_to_dbcol:
                            actual = normalized_to_dbcol[nc]
                            rename_map[c] = actual
                            matched_cols.append(actual)
                        else:
                            unmatched.append(c)

                    # Fuzzy fallback
                    db_tokens = {col: token_set(col) for col in existing_cols_original}
                    for c in unmatched:
                        ctokens = token_set(c)
                        best_col, best_score = None, 0.0
                        for col, dtokens in db_tokens.items():
                            if col in matched_cols:
                                continue
                            union = len(ctokens | dtokens) or 1
                            score = len(ctokens & dtokens) / union
                            if score > best_score:
                                best_col, best_score = col, score
                        if best_col and best_score >= 0.5:
                            rename_map[c] = best_col
                            matched_cols.append(best_col)

                    if not matched_cols:
                        st.error("No matching columns found between CSV and table.")
                        return

                    df_upload = df_upload.rename(columns=rename_map)
                    df_upload = df_upload[matched_cols]

                    # Show mapping
                    mapping_preview = pd.DataFrame([(k, v) for k, v in rename_map.items()],
                                                   columns=["CSV Header", "DB Column"])
                    st.caption("Column mapping (CSV ‚Üí DB):")
                    st.dataframe(mapping_preview, use_container_width=True, height=200)

                    # Required column validation
                    pk_info = inspector.get_pk_constraint(target_table)
                    pk_cols = set(pk_info.get('constrained_columns') or [])
                    required_missing = []
                    for c in cols_info:
                        name = c['name']
                        is_required = (not c.get('nullable', True)) and (c.get('default') is None)
                        is_auto_inc_pk = (name in pk_cols) and bool(c.get('autoincrement'))
                        if is_required and not is_auto_inc_pk and name not in df_upload.columns:
                            required_missing.append(name)
                    if required_missing:
                        st.error("Missing required columns in CSV: " + ", ".join(required_missing))
                        return

                    db_col_types = {c['name']: c['type'] for c in cols_info}

                    def coerce_value(value, coltype):
                        if value is None or (isinstance(value, str) and value.strip() == ""):
                            return None
                        try:
                            if isinstance(coltype, (satypes.Boolean,)):
                                if isinstance(value, str):
                                    v = value.strip().lower()
                                    if v in ("yes", "true", "1", "y", "t", "completed"):
                                        return True
                                    if v in ("no", "false", "0", "n", "f", "pending"):
                                        return False
                                return bool(int(value))
                            if isinstance(coltype, (satypes.Integer, satypes.BigInteger, satypes.SmallInteger)):
                                return int(float(value))
                            if isinstance(coltype, (satypes.Float,)):
                                return float(value)
                            if isinstance(coltype, (satypes.Numeric,)):
                                return Decimal(str(value))
                            if isinstance(coltype, (satypes.DateTime,)):
                                ts = pd.to_datetime(value, errors='coerce', utc=True)
                                if pd.isna(ts): return None
                                try: return ts.tz_convert(None).to_pydatetime()
                                except: return ts.tz_localize(None).to_pydatetime()
                            if isinstance(coltype, (satypes.Date,)):
                                dt = pd.to_datetime(value, errors='coerce', utc=True)
                                if pd.isna(dt): return None
                                try: return dt.tz_convert(None).date()
                                except: return dt.tz_localize(None).date()
                            if isinstance(coltype, (satypes.Time,)):
                                tt = pd.to_datetime(value, errors='coerce')
                                if pd.isna(tt): return None
                                return tt.time()
                        except Exception:
                            return None
                        return str(value)

                    # Chunked insert
                    total_rows = len(df_upload)
                    chunk_size = 500
                    num_chunks = (total_rows + chunk_size - 1) // chunk_size
                    progress = st.progress(0, text=f"Inserting 0/{total_rows} rows...")
                    inserted_rows = 0

                    for chunk_index in range(num_chunks):
                        start = chunk_index * chunk_size
                        end = min(start + chunk_size, total_rows)
                        df_chunk = df_upload.iloc[start:end]

                        if skip_empty_rows:
                            def non_empty_count(row):
                                return sum(1 for val in row if pd.notnull(val) and str(val).strip() != "")
                            mask = df_chunk.apply(non_empty_count, axis=1) >= min_populated_fields
                            df_chunk = df_chunk[mask]

                        try:
                            records = []
                            for _, row in df_chunk.iterrows():
                                rec = {}
                                for col in df_chunk.columns:
                                    coltype = db_col_types.get(col)
                                    rec[col] = coerce_value(row[col], coltype)
                                populated_after = sum(1 for v in rec.values() if v not in (None, ""))
                                if (not skip_empty_rows) or populated_after >= min_populated_fields:
                                    records.append(rec)

                            if records:
                                md = MetaData()
                                table_obj = Table(target_table, md, autoload_with=engine)
                                ins = mysql_insert(table_obj)
                                if skip_duplicates:
                                    ins = ins.prefix_with('IGNORE')
                                with engine.begin() as conn_chunk:
                                    conn_chunk.execute(ins, records)
                                inserted_rows += len(records)
                        except SQLAlchemyError as e:
                            st.warning(f"Chunk {chunk_index+1}/{num_chunks} failed: {e}")

                        progress.progress(min(inserted_rows / max(total_rows, 1), 1.0),
                                          text=f"Inserting {inserted_rows}/{total_rows} rows...")
                        time.sleep(0.01)

                    if inserted_rows:
                        st.success(f"‚úÖ Inserted {inserted_rows} rows into `{target_table}`")
                    else:
                        st.error("‚ùå No rows were inserted.")
                except SQLAlchemyError as e:
                    st.error(f"‚ùå Failed to insert data: {e}")

if __name__ == "__main__":
    main()
